---
sidebar_position: 2
title: "Voice-to-Action: OpenAI Whisper Integration"
---

# Lesson 1: Voice-to-Action: OpenAI Whisper Integration

## Overview

This lesson focuses on implementing voice recognition capabilities using OpenAI Whisper for Physical AI systems. We'll explore how to capture, process, and interpret voice commands to control humanoid robots and other Physical AI systems.

## Learning Objectives

By the end of this lesson, you will be able to:
- Set up OpenAI Whisper for real-time voice recognition
- Integrate Whisper with ROS 2 for voice command processing
- Implement voice activity detection and noise filtering
- Process voice commands for robotic action execution
- Handle multiple languages and accents in voice recognition

## Introduction to Voice Recognition in Physical AI

Voice recognition is a critical component of natural human-robot interaction in Physical AI systems. OpenAI Whisper provides state-of-the-art speech recognition capabilities that can be leveraged for:

1. **Natural Command Interface**: Allow users to control robots using natural language
2. **Accessibility**: Enable interaction for users with mobility limitations
3. **Hands-Free Operation**: Control robots without physical interfaces
4. **Multilingual Support**: Support multiple languages and dialects

## OpenAI Whisper Setup and Installation

### Prerequisites

Before installing Whisper, ensure you have:

```bash
# Python 3.8 or higher
python3 --version

# Required packages
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Additional dependencies
pip install openai-whisper faster-whisper
```

### Whisper Installation

Install Whisper with GPU support for better performance:

```bash
# Install Whisper with GPU support
pip install openai-whisper

# For faster processing, install faster-whisper
pip install faster-whisper
```

## Basic Whisper Implementation

### Simple Voice Recognition

Start with a basic Whisper implementation:

```python
import whisper
import torch
import numpy as np
import sounddevice as sd
from scipy.io.wavfile import write
import tempfile
import os

class WhisperVoiceRecognizer:
    def __init__(self, model_size="base"):
        # Check for GPU availability
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")

        # Load Whisper model
        self.model = whisper.load_model(model_size).to(self.device)

    def record_audio(self, duration=5, sample_rate=16000):
        """Record audio from microphone"""
        print(f"Recording {duration} seconds of audio...")
        audio_data = sd.rec(
            int(duration * sample_rate),
            samplerate=sample_rate,
            channels=1,
            dtype='float32'
        )
        sd.wait()  # Wait until recording is finished
        return audio_data.flatten()

    def transcribe_audio(self, audio_data):
        """Transcribe audio using Whisper"""
        # Convert audio to the format expected by Whisper
        audio_tensor = torch.from_numpy(audio_data).to(self.device)

        # Transcribe the audio
        result = self.model.transcribe(audio_tensor)
        return result["text"]

    def record_and_transcribe(self, duration=5):
        """Record and transcribe audio in one step"""
        audio = self.record_audio(duration)
        transcription = self.transcribe_audio(audio)
        return transcription

# Example usage
recognizer = WhisperVoiceRecognizer()
command = recognizer.record_and_transcribe(duration=5)
print(f"Recognized command: {command}")
```

### Optimized Whisper with faster-whisper

For better performance, use faster-whisper:

```python
from faster_whisper import WhisperModel
import sounddevice as sd
import numpy as np

class OptimizedWhisperRecognizer:
    def __init__(self, model_size="base", device="auto"):
        # Initialize faster Whisper model
        self.model = WhisperModel(
            model_size,
            device=device,
            compute_type="float16"  # Use float16 for better performance
        )

    def transcribe_audio(self, audio_path_or_data):
        """Transcribe audio with faster-whisper"""
        segments, info = self.model.transcribe(
            audio_path_or_data,
            beam_size=5,
            language="en"  # Specify language for better accuracy
        )

        transcription = " ".join([segment.text for segment in segments])
        return transcription.strip()

    def transcribe_with_timing(self, audio_path_or_data):
        """Transcribe with timing information"""
        segments, info = self.model.transcribe(
            audio_path_or_data,
            beam_size=5,
            word_timestamps=True
        )

        result = {
            "text": "",
            "segments": [],
            "language": info.language
        }

        for segment in segments:
            result["segments"].append({
                "text": segment.text,
                "start": segment.start,
                "end": segment.end,
                "words": [{"word": w.word, "start": w.start, "end": w.end}
                         for w in segment.words] if hasattr(segment, 'words') else []
            })
            result["text"] += segment.text + " "

        result["text"] = result["text"].strip()
        return result
```

## ROS 2 Integration

### Voice Command Node

Create a ROS 2 node that integrates Whisper for voice recognition:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from std_msgs.msg import Bool
from audio_common_msgs.msg import AudioData
import threading
import queue
import time

class VoiceCommandNode(Node):
    def __init__(self):
        super().__init__('voice_command_node')

        # Initialize Whisper recognizer
        self.whisper_recognizer = OptimizedWhisperRecognizer()

        # Publishers
        self.command_pub = self.create_publisher(String, '/voice_command', 10)
        self.listening_pub = self.create_publisher(Bool, '/voice_listening_status', 10)

        # Subscribers
        self.audio_sub = self.create_subscription(
            AudioData, '/audio_input', self.audio_callback, 10
        )

        # Parameters
        self.declare_parameter('voice_activation_keywords', ['hey robot', 'hello robot'])
        self.declare_parameter('command_timeout', 5.0)
        self.declare_parameter('min_silence_duration', 0.5)

        # Voice recognition state
        self.is_listening = False
        self.audio_buffer = queue.Queue()
        self.activation_keywords = self.get_parameter('voice_activation_keywords').value

        # Start voice processing thread
        self.voice_thread = threading.Thread(target=self.process_voice_commands, daemon=True)
        self.voice_thread.start()

        self.get_logger().info("Voice Command Node initialized")

    def audio_callback(self, msg):
        """Handle incoming audio data"""
        if self.is_listening:
            # Add audio data to buffer for processing
            self.audio_buffer.put(msg.data)

    def check_activation_phrase(self, transcription):
        """Check if transcription contains activation phrase"""
        transcription_lower = transcription.lower()
        for keyword in self.activation_keywords:
            if keyword.lower() in transcription_lower:
                return True
        return False

    def process_voice_commands(self):
        """Process voice commands in a separate thread"""
        while rclpy.ok():
            try:
                # Check for activation phrase first
                activation_audio = self.record_audio_segment(3)  # 3 seconds for activation
                activation_text = self.whisper_recognizer.transcribe_audio(activation_audio)

                if self.check_activation_phrase(activation_text):
                    self.get_logger().info(f"Activation phrase detected: {activation_text}")

                    # Publish listening status
                    listening_msg = Bool()
                    listening_msg.data = True
                    self.listening_pub.publish(listening_msg)

                    # Record command after activation
                    command_audio = self.record_audio_segment(5)  # 5 seconds for command
                    command_text = self.whisper_recognizer.transcribe_audio(command_audio)

                    # Publish command
                    cmd_msg = String()
                    cmd_msg.data = command_text.strip()
                    self.command_pub.publish(cmd_msg)

                    self.get_logger().info(f"Command recognized: {command_text}")

                    # Publish listening status (not listening anymore)
                    listening_msg.data = False
                    self.listening_pub.publish(listening_msg)

            except Exception as e:
                self.get_logger().error(f"Error in voice processing: {e}")
                time.sleep(0.1)  # Brief pause to prevent busy waiting

    def record_audio_segment(self, duration):
        """Record a segment of audio"""
        # This would interface with audio input
        # Implementation depends on your audio source
        pass
```

### Voice Activity Detection

Implement voice activity detection to improve efficiency:

```python
import webrtcvad
import collections
import pyaudio

class VoiceActivityDetector:
    def __init__(self, sample_rate=16000, frame_duration=30):
        self.sample_rate = sample_rate
        self.frame_duration = frame_duration
        self.vad = webrtcvad.Vad(2)  # Aggressiveness level 2

        # Audio frame size
        self.frame_size = int(sample_rate * frame_duration / 1000)

        # Buffer for voice detection
        self.ring_buffer = collections.deque(maxlen=30)  # 30 frames = 900ms

    def is_speech(self, audio_frame):
        """Check if audio frame contains speech"""
        try:
            return self.vad.is_speech(audio_frame, self.sample_rate)
        except:
            return False

    def detect_voice_activity(self, audio_data):
        """Detect voice activity in continuous audio stream"""
        speech_frames = []
        voiced_frames = 0

        # Split audio into frames
        for i in range(0, len(audio_data), self.frame_size):
            frame = audio_data[i:i + self.frame_size]

            # Pad frame if necessary
            if len(frame) < self.frame_size:
                frame = frame + b'\x00' * (self.frame_size - len(frame))

            if self.is_speech(frame):
                speech_frames.append(frame)
                voiced_frames += 1
            else:
                # Reset if too much silence
                if voiced_frames > 0:
                    self.ring_buffer.append((frame, False))

        # Return True if sufficient speech detected
        return len(speech_frames) > 0
```

## Advanced Voice Processing

### Noise Reduction and Audio Preprocessing

Implement audio preprocessing for better recognition:

```python
import webrtcvad
import collections
import numpy as np
from scipy import signal

class AudioPreprocessor:
    def __init__(self):
        # Parameters for audio preprocessing
        self.sample_rate = 16000
        self.noise_threshold = 0.01
        self.silence_threshold = 0.001

    def denoise_audio(self, audio_data):
        """Apply noise reduction to audio"""
        # Convert to numpy array
        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0

        # Apply spectral gating for noise reduction
        # This is a simplified version - in practice, use more sophisticated methods
        audio_denoised = self.spectral_gate(audio_array)

        # Convert back to int16
        audio_denoised = np.clip(audio_denoised * 32768.0, -32768, 32767).astype(np.int16)

        return audio_denoised.tobytes()

    def spectral_gate(self, audio):
        """Simple spectral gating for noise reduction"""
        # Calculate noise profile from beginning of audio
        noise_profile = np.mean(np.abs(audio[:int(0.5 * self.sample_rate)]))

        # Apply spectral gate
        magnitude = np.abs(audio)
        mask = magnitude > (noise_profile * 2)  # Threshold at 2x noise level

        return audio * mask.astype(float)

    def normalize_audio(self, audio_data):
        """Normalize audio amplitude"""
        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0

        # Normalize to -1 to 1 range
        normalized = audio_array / np.max(np.abs(audio_array))

        # Convert back to int16
        normalized = np.clip(normalized * 32768.0, -32768, 32767).astype(np.int16)

        return normalized.tobytes()

    def preprocess_audio(self, audio_data):
        """Complete audio preprocessing pipeline"""
        # Denoise
        denoised = self.denoise_audio(audio_data)

        # Normalize
        normalized = self.normalize_audio(denoised)

        return normalized
```

### Multi-language Support

Configure Whisper for multiple languages:

```python
class MultilingualVoiceRecognizer:
    def __init__(self):
        # Load models for different languages
        self.models = {}
        self.supported_languages = {
            'en': 'english',
            'es': 'spanish',
            'fr': 'french',
            'de': 'german',
            'it': 'italian',
            'pt': 'portuguese'
        }

        # Load base model
        self.base_model = whisper.load_model("medium")

    def detect_language(self, audio_data):
        """Detect language in audio"""
        # Use the model's built-in language detection
        audio_tensor = torch.from_numpy(audio_data).to(self.base_model.device)
        result = self.base_model.detect_language(audio_tensor)
        return result

    def transcribe_multilingual(self, audio_data, target_language=None):
        """Transcribe audio with language detection"""
        if target_language:
            # Transcribe to specific language
            result = self.base_model.transcribe(
                audio_data,
                language=target_language,
                task="transcribe"
            )
        else:
            # Auto-detect language
            result = self.base_model.transcribe(audio_data)

        return result["text"]
```

## Voice Command Processing

### Command Parsing and Interpretation

Process recognized voice commands for robotic action:

```python
import re
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class ParsedCommand:
    action: str
    parameters: dict
    confidence: float

class VoiceCommandParser:
    def __init__(self):
        # Define command patterns
        self.command_patterns = {
            'move': [
                r'move (?P<direction>\w+) (?P<distance>[\d.]+) meters?',
                r'go (?P<direction>\w+) (?P<distance>[\d.]+) (m|meters?)',
                r'walk (?P<direction>\w+) (?P<distance>[\d.]+) (m|meters?)'
            ],
            'rotate': [
                r'turn (?P<direction>left|right) (?P<angle>[\d.]+) degrees?',
                r'rotate (?P<direction>left|right) (?P<angle>[\d.]+) degrees?',
                r'spin (?P<direction>left|right) (?P<angle>[\d.]+) degrees?'
            ],
            'greet': [
                r'(say hello|greet|hello)',
                r'(wave|waving)',
                r'greeting'
            ],
            'pick': [
                r'pick up (?P<object>\w+)',
                r'grab (?P<object>\w+)',
                r'take (?P<object>\w+)'
            ]
        }

    def parse_command(self, text: str) -> Optional[ParsedCommand]:
        """Parse natural language command into structured format"""
        text_lower = text.lower().strip()

        for action, patterns in self.command_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, text_lower)
                if match:
                    parameters = match.groupdict()
                    # Convert numeric strings to numbers
                    for key, value in parameters.items():
                        try:
                            parameters[key] = float(value)
                        except ValueError:
                            pass  # Keep as string if not numeric

                    return ParsedCommand(
                        action=action,
                        parameters=parameters,
                        confidence=0.9  # High confidence for pattern match
                    )

        # If no pattern matches, return None or use LLM for complex parsing
        return None

    def parse_with_llm(self, text: str, llm_client) -> ParsedCommand:
        """Use LLM for complex command parsing"""
        prompt = f"""
        Parse the following natural language command for a robot:
        "{text}"

        Return the action and parameters in JSON format:
        {{
            "action": "action_name",
            "parameters": {{
                "param1": "value1",
                "param2": "value2"
            }},
            "confidence": 0.0-1.0
        }}
        """

        # Call LLM to parse complex commands
        response = llm_client.generate(prompt)
        # Parse JSON response
        parsed_data = json.loads(response)

        return ParsedCommand(
            action=parsed_data['action'],
            parameters=parsed_data['parameters'],
            confidence=parsed_data['confidence']
        )
```

## Hands-on Activity: Voice Command Integration

1. Install OpenAI Whisper on your system
2. Create a basic voice recognition node in ROS 2
3. Implement voice activity detection
4. Add audio preprocessing for noise reduction
5. Test voice command recognition with simple robot commands
6. Integrate with a simulated robot to execute voice commands

## Summary

Voice recognition using OpenAI Whisper provides a natural interface for controlling Physical AI systems. By integrating Whisper with ROS 2 and implementing proper audio preprocessing, we can create robust voice-to-action systems for humanoid robots and other Physical AI applications.

## Next Steps

In the next lesson, we'll explore cognitive planning using LLMs to translate natural language commands into sequences of robotic actions.