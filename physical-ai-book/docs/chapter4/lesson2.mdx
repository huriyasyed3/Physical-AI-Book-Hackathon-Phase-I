---
sidebar_position: 3
title: "Cognitive Planning: LLMs for Natural Language to ROS Actions"
---

# Lesson 2: Cognitive Planning: LLMs for Natural Language to ROS Actions

## Overview

This lesson explores how to use Large Language Models (LLMs) for cognitive planning in Physical AI systems. We'll focus on translating high-level natural language commands into sequences of ROS 2 actions that can be executed by robots, particularly humanoid robots.

## Learning Objectives

By the end of this lesson, you will be able to:
- Integrate LLMs for cognitive planning and task decomposition
- Create prompt engineering strategies for robotic action planning
- Implement natural language understanding for complex robot tasks
- Design action sequences from high-level commands
- Handle ambiguous or complex natural language requests

## Introduction to Cognitive Planning in Physical AI

Cognitive planning bridges the gap between high-level human commands and low-level robotic actions. This involves:

1. **Natural Language Understanding**: Interpreting human commands
2. **Task Decomposition**: Breaking complex tasks into simple actions
3. **Action Sequencing**: Creating executable action sequences
4. **Context Awareness**: Understanding the environment and constraints
5. **Error Handling**: Managing unexpected situations during execution

## LLM Integration for Cognitive Planning

### Setting Up LLM Integration

Integrate LLMs for cognitive planning:

```python
import openai
import json
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

@dataclass
class ActionStep:
    action_type: str  # 'move', 'rotate', 'gripper', 'wait', etc.
    parameters: Dict[str, Any]
    description: str
    estimated_duration: float  # in seconds

class CognitivePlanner:
    def __init__(self, api_key: str, model: str = "gpt-4-turbo"):
        openai.api_key = api_key
        self.model = model
        self.action_library = self._initialize_action_library()

    def _initialize_action_library(self) -> Dict[str, Any]:
        """Initialize the library of available robot actions"""
        return {
            "navigation": {
                "move_forward": {
                    "description": "Move robot forward by specified distance",
                    "parameters": {"distance": "float (meters)"},
                    "ros_topic": "/cmd_vel",
                    "example": "Move forward 2 meters"
                },
                "move_backward": {
                    "description": "Move robot backward by specified distance",
                    "parameters": {"distance": "float (meters)"},
                    "ros_topic": "/cmd_vel",
                    "example": "Move backward 1 meter"
                },
                "rotate": {
                    "description": "Rotate robot by specified angle",
                    "parameters": {"angle": "float (degrees)", "direction": "str (left/right)"},
                    "ros_topic": "/cmd_vel",
                    "example": "Turn left 90 degrees"
                },
                "navigate_to": {
                    "description": "Navigate to a specific location",
                    "parameters": {"x": "float", "y": "float", "theta": "float"},
                    "ros_topic": "/navigate_to_pose",
                    "example": "Go to the kitchen"
                }
            },
            "manipulation": {
                "pick_object": {
                    "description": "Pick up an object",
                    "parameters": {"object_name": "str", "location": "str"},
                    "ros_topic": "/pick_action",
                    "example": "Pick up the red cup"
                },
                "place_object": {
                    "description": "Place an object at a location",
                    "parameters": {"object_name": "str", "location": "str"},
                    "ros_topic": "/place_action",
                    "example": "Place the cup on the table"
                },
                "gripper_control": {
                    "description": "Control robot gripper",
                    "parameters": {"command": "str (open/close)", "force": "float (0-100)"},
                    "ros_topic": "/gripper_command",
                    "example": "Open gripper"
                }
            },
            "perception": {
                "detect_object": {
                    "description": "Detect specific objects in the environment",
                    "parameters": {"object_type": "str"},
                    "ros_topic": "/object_detection",
                    "example": "Find the blue ball"
                },
                "scan_environment": {
                    "description": "Scan and map the environment",
                    "parameters": {},
                    "ros_topic": "/scan",
                    "example": "Look around"
                }
            }
        }

    async def plan_from_command(self, command: str, robot_context: Dict[str, Any]) -> List[ActionStep]:
        """Generate action sequence from natural language command"""
        prompt = self._create_planning_prompt(command, robot_context)

        try:
            response = await openai.ChatCompletion.acreate(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._get_system_prompt()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )

            action_sequence = self._parse_action_sequence(response.choices[0].message.content)
            return action_sequence

        except Exception as e:
            print(f"Error in cognitive planning: {e}")
            return []

    def _create_planning_prompt(self, command: str, robot_context: Dict[str, Any]) -> str:
        """Create the prompt for cognitive planning"""
        context_str = json.dumps(robot_context, indent=2)
        actions_str = json.dumps(self.action_library, indent=2)

        prompt = f"""
        You are a cognitive planner for a Physical AI robot. Your task is to decompose natural language commands into sequences of specific robotic actions.

        Current robot context:
        {context_str}

        Available actions:
        {actions_str}

        Command: "{command}"

        Please decompose this command into a sequence of specific actions that the robot can execute. Return the result as a JSON array with the following structure:
        [
            {{
                "action_type": "action_name",
                "parameters": {{"param1": "value1", "param2": "value2"}},
                "description": "Brief description of what this action does",
                "estimated_duration": 2.5
            }}
        ]

        Be specific about parameters and consider the current robot context when planning.
        """

        return prompt

    def _get_system_prompt(self) -> str:
        """Get the system prompt for the LLM"""
        return """
        You are an expert cognitive planner for Physical AI robots. Your role is to decompose high-level natural language commands into specific, executable robotic actions.

        Rules:
        1. Always return a valid JSON array of actions
        2. Each action must have: action_type, parameters, description, and estimated_duration
        3. Use only actions from the provided action library
        4. Consider robot constraints and environment context
        5. Break complex tasks into simple, sequential actions
        6. Include necessary perception actions before manipulation
        7. Add safety checks and validations where appropriate
        """
```

### Advanced Planning with Memory and Context

Implement memory and context awareness:

```python
from datetime import datetime
import pickle
import os

class ContextAwarePlanner(CognitivePlanner):
    def __init__(self, api_key: str, model: str = "gpt-4-turbo", memory_file: str = "robot_memory.pkl"):
        super().__init__(api_key, model)
        self.memory_file = memory_file
        self.conversation_history = []
        self.robot_memory = self._load_memory()

    def _load_memory(self) -> Dict[str, Any]:
        """Load robot memory from file"""
        if os.path.exists(self.memory_file):
            with open(self.memory_file, 'rb') as f:
                return pickle.load(f)
        return {
            "locations": {},
            "objects": {},
            "tasks_completed": [],
            "preferences": {},
            "last_interactions": []
        }

    def _save_memory(self):
        """Save robot memory to file"""
        with open(self.memory_file, 'wb') as f:
            pickle.dump(self.robot_memory, f)

    def update_memory(self, interaction: Dict[str, Any]):
        """Update robot memory with new interaction"""
        self.robot_memory["last_interactions"].append({
            "timestamp": datetime.now().isoformat(),
            "interaction": interaction
        })

        # Keep only recent interactions (last 100)
        self.robot_memory["last_interactions"] = self.robot_memory["last_interactions"][-100:]

        self._save_memory()

    async def plan_with_context(self, command: str, user_id: str = "default_user") -> List[ActionStep]:
        """Plan with full context including memory"""
        # Get user-specific context
        user_context = self._get_user_context(user_id)

        # Get current robot state
        robot_state = await self._get_robot_state()

        # Combine contexts
        full_context = {
            "user_context": user_context,
            "robot_state": robot_state,
            "environment": await self._get_environment_state(),
            "memory": self.robot_memory,
            "time": datetime.now().isoformat()
        }

        # Generate plan
        action_sequence = await self.plan_from_command(command, full_context)

        # Update memory with this interaction
        self.update_memory({
            "command": command,
            "plan": [action.__dict__ for action in action_sequence],
            "user_id": user_id
        })

        return action_sequence

    def _get_user_context(self, user_id: str) -> Dict[str, Any]:
        """Get user-specific context"""
        return {
            "user_id": user_id,
            "preferences": self.robot_memory.get("preferences", {}).get(user_id, {}),
            "interaction_history": self._get_recent_interactions(user_id)
        }

    async def _get_robot_state(self) -> Dict[str, Any]:
        """Get current robot state from ROS"""
        # This would interface with ROS to get current robot state
        # For now, return a mock state
        return {
            "position": {"x": 0.0, "y": 0.0, "theta": 0.0},
            "battery_level": 85.0,
            "gripper_status": "open",
            "current_task": None
        }

    async def _get_environment_state(self) -> Dict[str, Any]:
        """Get current environment state"""
        # This would interface with perception systems
        return {
            "objects_detected": [],
            "navigable_areas": [],
            "obstacles": [],
            "lighting_conditions": "normal"
        }

    def _get_recent_interactions(self, user_id: str, limit: int = 10) -> List[Dict]:
        """Get recent interactions for a user"""
        user_interactions = []
        for interaction in reversed(self.robot_memory["last_interactions"]):
            if interaction.get("user_id") == user_id:
                user_interactions.append(interaction)
                if len(user_interactions) >= limit:
                    break
        return user_interactions
```

## ROS 2 Action Execution

### Action Execution Node

Create a ROS 2 node that executes planned actions:

```python
import rclpy
from rclpy.node import Node
from rclpy.action import ActionClient
from rclpy.qos import QoSProfile
from std_msgs.msg import String, Bool
from geometry_msgs.msg import Twist
from nav2_msgs.action import NavigateToPose
from builtin_interfaces.msg import Duration

class ActionExecutorNode(Node):
    def __init__(self):
        super().__init__('action_executor_node')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.status_pub = self.create_publisher(String, '/action_status', 10)

        # Action clients
        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')

        # Subscribers
        self.plan_sub = self.create_subscription(
            String, '/action_plan', self.plan_callback, 10
        )

        # Parameters
        self.declare_parameter('max_action_duration', 30.0)
        self.declare_parameter('action_retry_count', 3)

        # Action execution state
        self.current_plan = []
        self.current_action_index = 0
        self.is_executing = False

        self.get_logger().info("Action Executor Node initialized")

    def plan_callback(self, msg):
        """Handle incoming action plan"""
        try:
            plan_data = json.loads(msg.data)
            self.current_plan = plan_data
            self.current_action_index = 0
            self.is_executing = True

            self.get_logger().info(f"Received plan with {len(plan_data)} actions")

            # Start executing the plan
            self.execute_next_action()

        except json.JSONDecodeError as e:
            self.get_logger().error(f"Invalid plan format: {e}")

    def execute_next_action(self):
        """Execute the next action in the plan"""
        if not self.is_executing or self.current_action_index >= len(self.current_plan):
            self.is_executing = False
            self.publish_status("Plan completed")
            return

        action = self.current_plan[self.current_action_index]

        self.get_logger().info(f"Executing action {self.current_action_index + 1}: {action['action_type']}")

        # Execute based on action type
        if action['action_type'] == 'move_forward':
            self.execute_move_action(action, forward=True)
        elif action['action_type'] == 'move_backward':
            self.execute_move_action(action, forward=False)
        elif action['action_type'] == 'rotate':
            self.execute_rotate_action(action)
        elif action['action_type'] == 'navigate_to':
            self.execute_navigation_action(action)
        else:
            self.get_logger().warn(f"Unknown action type: {action['action_type']}")
            self.current_action_index += 1
            self.execute_next_action()

    def execute_move_action(self, action, forward=True):
        """Execute a move action"""
        distance = action['parameters']['distance']

        # Calculate time needed (assuming constant speed)
        speed = 0.5  # m/s
        duration = distance / speed

        twist_msg = Twist()
        twist_msg.linear.x = speed if forward else -speed

        # Publish command
        self.cmd_vel_pub.publish(twist_msg)

        # Schedule stop after calculated duration
        timer = self.create_timer(duration, lambda: self.stop_robot())

        # Schedule next action after movement completes
        self.create_timer(duration + 0.5, self.execute_next_action)

    def execute_rotate_action(self, action):
        """Execute a rotation action"""
        angle = action['parameters']['angle']
        direction = action['parameters'].get('direction', 'left')

        # Convert angle to radians
        angle_rad = angle * 3.14159 / 180.0

        # Calculate rotation time (assuming constant angular velocity)
        angular_speed = 0.5  # rad/s
        duration = abs(angle_rad) / angular_speed

        twist_msg = Twist()
        twist_msg.angular.z = angular_speed if direction == 'left' else -angular_speed

        # Publish command
        self.cmd_vel_pub.publish(twist_msg)

        # Schedule stop after calculated duration
        timer = self.create_timer(duration, lambda: self.stop_robot())

        # Schedule next action after rotation completes
        self.create_timer(duration + 0.5, self.execute_next_action)

    def execute_navigation_action(self, action):
        """Execute a navigation action"""
        x = action['parameters']['x']
        y = action['parameters']['y']
        theta = action['parameters'].get('theta', 0.0)

        # Wait for action server
        self.nav_client.wait_for_server()

        # Create goal
        goal_msg = NavigateToPose.Goal()
        goal_msg.pose.header.frame_id = 'map'
        goal_msg.pose.pose.position.x = x
        goal_msg.pose.pose.position.y = y
        goal_msg.pose.pose.orientation.z = theta

        # Send goal
        future = self.nav_client.send_goal_async(goal_msg)
        future.add_done_callback(self.navigation_done_callback)

    def navigation_done_callback(self, future):
        """Handle navigation completion"""
        goal_result = future.result()

        if goal_result.accepted:
            self.get_logger().info("Navigation goal accepted")
            # Wait for result
            result_future = goal_result.get_result_async()
            result_future.add_done_callback(self.navigation_result_callback)
        else:
            self.get_logger().error("Navigation goal rejected")
            self.current_action_index += 1
            self.execute_next_action()

    def navigation_result_callback(self, future):
        """Handle navigation result"""
        result = future.result().result

        if result:
            self.get_logger().info("Navigation completed successfully")
        else:
            self.get_logger().error("Navigation failed")

        # Move to next action
        self.current_action_index += 1
        self.execute_next_action()

    def stop_robot(self):
        """Stop robot movement"""
        stop_msg = Twist()
        self.cmd_vel_pub.publish(stop_msg)

    def publish_status(self, status):
        """Publish action execution status"""
        status_msg = String()
        status_msg.data = status
        self.status_pub.publish(status_msg)
```

## Handling Ambiguous Commands

### Command Disambiguation

Handle ambiguous or unclear commands:

```python
class CommandDisambiguator:
    def __init__(self, llm_client):
        self.llm_client = llm_client

    async def disambiguate_command(self, command: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Ask clarifying questions for ambiguous commands"""
        prompt = f"""
        Analyze the following command for ambiguity:
        "{command}"

        Current context:
        {json.dumps(context, indent=2)}

        Identify what information is missing or unclear in this command.
        If the command is clear, return "CLEAR".
        If it's ambiguous, return a JSON object with:
        {{
            "ambiguous": true,
            "issues": ["issue1", "issue2"],
            "questions": ["question1", "question2"]
        }}
        """

        try:
            response = await self.llm_client.acreate(
                model="gpt-4-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500
            )

            result = response.choices[0].message.content.strip()

            if result == "CLEAR":
                return {"ambiguous": False}

            # Parse the JSON response
            return json.loads(result)

        except Exception as e:
            print(f"Error in disambiguation: {e}")
            return {"ambiguous": False}  # Default to assuming it's clear

    async def get_clarification(self, command: str, context: Dict[str, Any]) -> str:
        """Get clarification from user for ambiguous command"""
        disambiguation = await self.disambiguate_command(command, context)

        if not disambiguation.get("ambiguous"):
            return command

        questions = disambiguation.get("questions", [])
        if questions:
            # In a real system, this would prompt the user
            # For now, we'll simulate the clarification process
            clarification_prompt = f"""
            The command "{command}" is ambiguous. Please clarify based on these questions:
            {chr(10).join(f"- {q}" for q in questions)}

            Rephrase the command to be more specific.
            """

            # This would normally be handled by the voice interface
            # Returning the original command for now
            return command

        return command
```

## Error Handling and Recovery

### Robust Planning with Error Recovery

Implement error handling and recovery strategies:

```python
class RobustPlanner(CognitivePlanner):
    def __init__(self, api_key: str, model: str = "gpt-4-turbo"):
        super().__init__(api_key, model)
        self.error_recovery_strategies = self._initialize_recovery_strategies()

    def _initialize_recovery_strategies(self) -> Dict[str, Any]:
        """Initialize error recovery strategies"""
        return {
            "navigation_failure": [
                "Try alternative route",
                "Use different navigation parameters",
                "Request human assistance"
            ],
            "object_not_found": [
                "Expand search area",
                "Check different locations",
                "Ask for more specific description"
            ],
            "gripper_failure": [
                "Retry with different approach angle",
                "Adjust gripper force",
                "Try different grasp point"
            ]
        }

    async def plan_with_recovery(self, command: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate plan with built-in error recovery"""
        # First, create the main plan
        main_plan = await self.plan_from_command(command, context)

        # Add error recovery points
        enhanced_plan = await self._add_recovery_points(main_plan, context)

        return {
            "main_plan": main_plan,
            "recovery_plan": enhanced_plan,
            "fallback_options": await self._generate_fallbacks(command, context)
        }

    async def _add_recovery_points(self, plan: List[ActionStep], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Add recovery points to the plan"""
        enhanced_plan = []

        for i, action in enumerate(plan):
            enhanced_action = {
                "action": action,
                "recovery_points": [],
                "success_criteria": await self._define_success_criteria(action, context)
            }

            # Add recovery strategies based on action type
            recovery_strategies = await self._get_recovery_strategies(action, context)
            enhanced_action["recovery_points"] = recovery_strategies

            enhanced_plan.append(enhanced_action)

        return enhanced_plan

    async def _get_recovery_strategies(self, action: ActionStep, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Get recovery strategies for a specific action"""
        # Define recovery strategies based on action type
        if "navigate" in action.action_type.lower():
            return [
                {
                    "condition": "obstacle_detected",
                    "strategy": "find_alternative_route",
                    "parameters": {"max_detour": 2.0}
                },
                {
                    "condition": "goal_unreachable",
                    "strategy": "move_to_closest_reachable",
                    "parameters": {}
                }
            ]
        elif "pick" in action.action_type.lower():
            return [
                {
                    "condition": "object_not_found",
                    "strategy": "expand_search_area",
                    "parameters": {"search_radius": 1.0}
                },
                {
                    "condition": "grasp_failed",
                    "strategy": "adjust_approach",
                    "parameters": {"approach_angle": 15.0}
                }
            ]

        return []

    async def _define_success_criteria(self, action: ActionStep, context: Dict[str, Any]) -> Dict[str, Any]:
        """Define success criteria for an action"""
        if "navigate" in action.action_type.lower():
            return {
                "position_tolerance": 0.1,  # meters
                "orientation_tolerance": 0.1,  # radians
                "timeout": action.estimated_duration * 2
            }
        elif "pick" in action.action_type.lower():
            return {
                "object_detected": True,
                "gripper_force": {"min": 5.0, "max": 50.0},
                "timeout": action.estimated_duration * 3
            }

        return {"timeout": action.estimated_duration * 2}
```

## Integration with Vision Systems

### Vision-Language Integration

Combine vision and language for better understanding:

```python
class VisionLanguagePlanner:
    def __init__(self, llm_client, vision_client):
        self.llm_client = llm_client
        self.vision_client = vision_client

    async def plan_with_vision_context(self, command: str, image_data: bytes = None) -> List[ActionStep]:
        """Plan with visual context"""
        if image_data:
            # Analyze image to get visual context
            vision_analysis = await self.vision_client.analyze_image(image_data)

            # Combine with LLM planning
            prompt = f"""
            Natural language command: "{command}"

            Visual context from image analysis:
            {json.dumps(vision_analysis, indent=2)}

            Create a detailed action plan that considers both the command and the visual context.
            """
        else:
            # Just use the command
            prompt = f'Create an action plan for: "{command}"'

        # Generate plan using LLM
        response = await self.llm_client.acreate(
            model="gpt-4-vision-preview",  # Use vision-capable model if image provided
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1000
        )

        return self._parse_action_sequence(response.choices[0].message.content)
```

## Hands-on Activity: Cognitive Planning Implementation

1. Set up an LLM API key and integrate with your system
2. Create a cognitive planner that can interpret natural language commands
3. Implement action execution for a simulated robot
4. Add error handling and recovery mechanisms
5. Test with various natural language commands
6. Evaluate the effectiveness of the planning system

## Summary

Cognitive planning using LLMs enables Physical AI systems to understand and execute complex natural language commands. By combining language understanding with action execution and error recovery, we can create more intuitive and capable robot systems.

## Next Steps

In the next lesson, we'll implement the capstone project: The Autonomous Humanoid system that integrates all the concepts learned throughout the course.